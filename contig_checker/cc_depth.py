import pickle
from typing import List
from dataclasses import dataclass, field
import numpy as np
import pandas as pd
from logzero import logger


def load_dbdump(dbdump_fname):
    """
    Load DAZZ_DB ID, fasta header name, and sequence length for each read in a database.
    The dump file must be generated by a command `$ DBdump -r -h`.
    """

    seqs = {}
    with open(dbdump_fname, 'r') as f:
        for line in f:
            data = line.strip().split(' ')
            if data[0] == "R":
                dbid = int(data[1])
            elif data[0] == "H":
                header = data[2][1:]
            elif data[0] == "L":   
                length = int(data[3])
                seqs[dbid] = [header, length]

    seqs = pd.DataFrame.from_dict(seqs, orient="index", columns=("header", "length"))
    seqs.index.name = "dbid"
    return seqs


@dataclass(repr=False, eq=False)
class Mapping:
    """
    Class of a mapping of a read onto a contig.
    Mapping type is also determined based on the following features:

    * Number of split alignments: single or chain
    * Terminal                  : proper or clipped
    * Inclusion relationship    : contains, contained, start_dovetail, or end_dovetail

    Note that "contains" (a read contains a contig from end to end) can occur only when terminal is "proper".
    """

    # Initialization = Loading a "P" line in a ladump file
    contig_id: int
    read_id: int
    contig_len: int
    read_len: int
    strand: str
    split_type: str = field(default="single", init=False)   # "single" if not chain else "chain"
    break_starts: List[int] = field(default_factory=list, init=False)   # intervals of breaks between chains
    break_ends: List[int] = field(default_factory=list, init=False)

    def add_alignment(self, chain, contig_s, contig_e, read_s, read_e):
        """
        Add an alignment (which might be part of a chain) = Loading a "C" line in a ladump file.
        """

        if not chain:
            self.contig_s = contig_s
            self.contig_e = contig_e
            self.read_s = read_s
            self.read_e = read_e
        else:
            self.split_type = "chain"
            self.break_starts.append(self.contig_e - 1)   # TODO: 1 is necessary?
            self.break_ends.append(contig_s)
            self.contig_e = contig_e   # Update only end positions
            self.read_e = read_e
            
    def _check_type(self):
        if self.contig_s == 0 and self.contig_e == self.contig_len:
            self.terminal = "proper"
            self.map_type = "contains"
        elif self.contig_s == 0 and self.read_s > 0:
            self.map_type = "start_dovetail"
            self.terminal = "proper" if self.read_e == self.read_len else "clipped"
        elif self.contig_e == self.contig_len and self.read_e < self.read_len:
            self.map_type = "end_dovetail"
            self.terminal = "proper" if self.read_s == 0 else "clipped"
        else:
            self.map_type = "contained"
            self.terminal = "proper" if self.read_s == 0 and self.read_e == self.read_len else "clipped"
        
    def as_list(self):
        self._check_type()
        return [self.contig_id,
                self.read_id,
                self.strand,
                self.contig_s,
                self.contig_e - 1,
                self.split_type,
                self.terminal,
                self.map_type,
                self.break_starts,
                self.break_ends]


def load_ladump(ladump_fname, contigs, reads):
    """
    Load read mappings during categorizing their mapping types.
    The dump file must be generated by a command `$ LAdump -c` (after sorting the las file by `$ LAsort -a`).
    """

    mappings = {}
    index = 0
    with open(ladump_fname, 'r') as f:
        flag_first = True
        for line in f:
            data = line.strip().split(' ')
            if data[0] == "P":
                chain = True if data[4] == "-" else False
                if chain:
                    continue
                
                if flag_first:
                    flag_first = False
                else:
                    # Fix a mapping after all split alignments in a chain are added
                    mappings[index] = mapping.as_list()
                    index += 1
                
                # Prepare a new mapping
                contig_id, read_id, strand = int(data[1]), int(data[2]), data[3]
                mapping = Mapping(contig_id,
                                  read_id,
                                  contigs.loc[contig_id, "length"],
                                  reads.loc[read_id, "length"],
                                  strand)

            elif data[0] == "C":
                contig_s, contig_e, read_s, read_e = list(map(int, data[1:]))
                mapping.add_alignment(chain, contig_s, contig_e, read_s, read_e)

        mappings[index] = mapping.as_list()   # the last mapping

    mappings = pd.DataFrame.from_dict(mappings, orient="index",
                                      columns=("contig_id",
                                               "read_id",
                                               "strand",
                                               "contig_start",
                                               "contig_end",
                                               "split_type",
                                               "terminal_type",
                                               "map_type",
                                               "break_starts",
                                               "break_ends"))
    return mappings


def vectorize(pos, l):
    """
    List or list of list of int -> frequency vector of the values. l is the maximum value (= contig length).
    """

    ret = np.zeros(l, dtype=int)
    for p in pos:
        if isinstance(p, list):
            for pp in p:
                ret[pp] += 1
        else:
            ret[p] += 1
    return ret


def mappings_to_counts(mappings, contigs):   # XXX: mapped readがないcontigのrowが作られないのを直す(ex. contig id 16)
    """
    For each mapping type ("proper" or "clipped"), calculate the number of:

    * reads containing each contig from end to end (= contains; only "clipped")
    * dovetail overlaps at start/end of each contig
    * reads starting/ending at each position on each contig
    * chain breaks starting/ending at each position on each contig
    """

    # Split by contigs and mapping type (clipped or proper)
    gb = mappings.groupby(["contig_id", "terminal_type"])

    # contains, dovetail start/end reads
    c = gb.apply(lambda df: df[df["map_type"] == "contains"].shape[0])
    s = gb.apply(lambda df: df[df["map_type"] == "start_dovetail"].shape[0])
    e = gb.apply(lambda df: df[df["map_type"] == "end_dovetail"].shape[0])

    # start/end points of mapped reads
    sc = gb.apply(lambda df: vectorize(df[df["map_type"].isin(set(["contained", "end_dovetail"]))]["contig_start"],
                                       contigs.loc[df.name[0], "length"]))
    ec = gb.apply(lambda df: vectorize(df[df["map_type"].isin(set(["contained", "start_dovetail"]))]["contig_end"],
                                       contigs.loc[df.name[0], "length"]))

    # chain breaks
    sb = gb.apply(lambda df: vectorize(df[df["split_type"] == "chain"]["break_starts"],
                                       contigs.loc[df.name[0], "length"]))
    eb = gb.apply(lambda df: vectorize(df[df["split_type"] == "chain"]["break_ends"],
                                       contigs.loc[df.name[0], "length"]))

    return pd.DataFrame({"n_contains": c,
                         "n_dovetail_start": s,
                         "n_dovetail_end": e,
                         "array_n_start": sc,
                         "array_n_end": ec,
                         "array_n_break_start": sb,
                         "array_n_break_end": eb})


# Sum up start/end count around peaks (aggregated counts disappear except the peaks)
def aggregate_counts(a, count_threshold, window_size):
    x_spike = np.where(a >= count_threshold)[0]
    x_done = np.zeros(len(a))
    for x in x_spike:
        x_done[x] = a[x]   # ex. [0, 0, ..., 0, 5, 0, ..., 0, 8, 0, ..., 0, 0]
    while sum(x_done) > 0:
        max_pos = np.argmax(x_done)
        s = 0
        for i in range(max_pos - int(window_size / 2), max_pos + int(window_size / 2) + 1):
            s += a[i]
            a[i] = 0
            x_done[i] = 0
        a[max_pos] = s
    return np.where(a >= count_threshold)[0]


def find_spike(a_proper, a_all, count_threshold, window_size):
    x_proper = aggregate_counts(a_proper, count_threshold, window_size)
    x_all = aggregate_counts(a_all, count_threshold, window_size)
    #return sorted(list(set(x_proper) | set(x_all)))
    return (x_proper, x_all)


def counts_to_depth(counts):
    # depth
    d = (counts["n_dovetail_start"]   # at the start edge of the contig
         + counts["array_n_start"].apply(lambda s: s.cumsum())
         - counts["array_n_end"].apply(lambda s: s.cumsum()))
    # chain break depth
    db = (counts["array_n_break_start"].apply(lambda s: s.cumsum())
          - counts["array_n_break_end"].apply(lambda s: s.cumsum()))
    return pd.DataFrame({"depth": d, "break_depth": db})


def calc_depth(contig_db_prefix="CONTIGS",
               read_db_prefix="READS"):
    # Load the dump data and reformat them
    contigs = load_dbdump(f"{contig_db_prefix}.dbdump")
    contigs.to_pickle("contigs.pkl")
    #contigs = pickle.load(open("contigs.pkl", 'rb'))

    reads = load_dbdump(f"{read_db_prefix}.dbdump")
    reads.to_pickle("reads.pkl")
    #reads = pickle.load(open("reads.pkl", 'rb'))

    # Aggregate into a single table which summarizes mapped reads and their types
    mappings = load_ladump(f"{contig_db_prefix}.{read_db_prefix}.ladump", contigs, reads)
    mappings.to_pickle("mappings.pkl")
    #mappings = pickle.load(open("mappings.pkl", 'rb'))

    # Calculate frequencies of starting/ending positions of the mapped reads
    counts = mappings_to_counts(mappings, contigs)
    counts.to_pickle("counts.pkl")
    #counts = pickle.load(open("counts.pkl", 'rb'))

    # Calculate depth transition
    depths = counts_to_depth(counts)
    depths.to_pickle("depths.pkl")
